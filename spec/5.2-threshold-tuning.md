# 5.2 Threshold Tuning & Ablation

**Phase**: 4 - Validation & Tuning
**Week**: 11
**Files**: `tests/tuning/grid_search.cpp`, `docs/ablation_study.md`

## Purpose
Optimize classification thresholds and understand metric contributions.

## Grid Search

### Search Space
```cpp
planarity_threshold:         [0.5, 0.6, 0.7, 0.8, 0.9]
alpha_mean_threshold:        [0.7, 0.8, 0.85, 0.9, 0.95]
alpha_variance_threshold:    [0.05, 0.1, 0.15, 0.2]
normal_coherence_threshold:  [0.5, 0.6, 0.7, 0.8]
```

Total combinations: 5 × 5 × 4 × 4 = 400

### Optimization Goal
Maximize F1 score on validation set

### Implementation
```cpp
for each threshold combination:
    classify all test scenes
    compute F1 score
    record if best
```

## Ablation Study

Test classifier with subsets of metrics:

1. **PCA only** (no alpha, no normal)
2. **Alpha only** (no PCA, no normal)
3. **Normal only** (no PCA, no alpha)
4. **PCA + Alpha** (no normal)
5. **PCA + Normal** (no alpha)
6. **Alpha + Normal** (no PCA)
7. **All metrics** (baseline)

### Analysis
- Which metrics contribute most?
- Can any metric be removed without accuracy loss?
- Are metrics redundant?

## Failure Analysis

### Questions to Answer
- Which clusters are misclassified?
- What are their metric values?
- Is there a pattern by scene type?
- Are failures near decision boundary?

### Documentation
Create `docs/failure_cases.md` with:
- Examples of misclassified clusters
- Metric values for failures
- Hypotheses for why they failed
- Suggested improvements

## Target
- F1 score > 0.8 on test set
- Clear understanding of metric contributions

## Outputs
- `optimal_thresholds.json`
- `ablation_results.csv`
- `failure_cases.md`
