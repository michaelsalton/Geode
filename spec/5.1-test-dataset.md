# 5.1 Test Dataset & Ground Truth

**Phase**: 4 - Validation & Tuning
**Week**: 10
**Files**: `tests/evaluation/`, `data/test_scenes/`

## Purpose
Establish ground truth labels and evaluation metrics for classifier validation.

## Test Scenes

### Required Datasets
1. **Mip-NeRF 360** (5 scenes)
   - bicycle, garden, room, counter, kitchen

2. **DoD Archaeological Scans** (2-3 scenes)
   - Photogrammetry reconstructions

3. **Synthetic Scenes** (3-5 scenes)
   - Known ground truth (procedurally generated)

4. **Indoor Scenes** (2-3 scenes)
   - Deep Blending dataset or similar

### Scene Diversity
- Outdoor vs indoor
- Large scale vs small scale
- Clean geometry vs noisy reconstruction

## Ground Truth Labeling

### Manual Labeling Tool
- Brush tool to paint regions as mesh/Gaussian
- Or: Label cluster IDs after initial classification
- Guidelines:
  - Walls, floors, ceilings → mesh
  - Foliage, smoke, fuzzy regions → Gaussian
  - Complex thin structures → case-by-case

### Label Format
```json
{
  "scene": "bicycle.ply",
  "clusters": [
    {"id": 0, "label": "mesh", "confidence": 1.0},
    {"id": 1, "label": "gaussian", "confidence": 0.9}
  ]
}
```

## Evaluation Metrics

```cpp
struct EvaluationResults {
    float accuracy;           // correct / total
    float mesh_precision;     // TP / (TP + FP)
    float mesh_recall;        // TP / (TP + FN)
    float mesh_f1;           // harmonic mean
    float gaussian_precision;
    float gaussian_recall;
    float gaussian_f1;
};
```

## Implementation
- Evaluation script: `tests/evaluate_classifier.cpp`
- Compute metrics per scene and aggregate
- Generate confusion matrix

## Validation
- At least 5 scenes labeled
- Inter-rater agreement > 0.8 (if multiple labelers)
- Baseline accuracy established
